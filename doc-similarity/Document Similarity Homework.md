# Document Similarity Homework
## Job Description
课程“海量信息处理”的个人作业。
利用人民日报的语料，计算文档间的相似度，输出相似度矩阵。

## similary.py
不借助numpy等库，自己用python实现的基于tf-idf值比较的文档相似度计算。

主要流程:

- 遍历一遍文档，遍历过程中，可以得到每一个文档中每个单词的词频，以及所有单词的idf值，将这两个分别记录在tf\_list和df\_dict中。其中tf_list的每一个元素为一个dict，该dict的key是单词，value是词频。
- 做一次循环，将tf值更新为tf-idf值
- 计算两两文档的相似度。

优化的地方：
- 保存term frequency 和 doc frequency
- 用set保存每个doc对应的单词列表
- 在遍历一遍结束得到tf和idf后，计算tf－idf值并保存，而不是在求两个文档的相似度时再计算tf-idf
- 用numpy的numpy.empty()来初始化一个空的列表。
- 在做相似度计算时，由于每一个文档都会需要计算其norm值，因此，计算一次后，用一个dict保存下来。


目前总耗时:2636.6秒，相似度计算时间：2621.3秒

另外，我尝试做了一个小小的修改，在计算文档相似度时，利用numpy提供的dot来计算内积来计算相似度，整个计算相似度的时常由2636.6秒减少为2236.9秒，有10%左右的性能提升。

## similarity\_with\_library.py
利用numpy及sklearn等库，实现相似度计算。

sklearn中直接实现了TfIdfVectorizer，内部利用numpy数组来存储稀疏矩阵以及实现相关的矩阵运算的优化。用库计算相似度仅用了44秒，总耗时60秒。可见自己在实现时还有很多可以优化的点。

## Future work
看了一下sklearn.metrics.pairwise的cosine_similary的代码，发现它在计算时，先把每个向量做了normalizetion，然后在每次计算时只要计算内积即可，这是一个优化点。

另外，现在自己实现的版本，每计算一对文档的相似度，会临时生成两个向量来进行计算，这就有大量的申请新内存的过程。而在sklearn的实现中，是利用sparse array来存储大矩阵，即利用词袋模型，一次性生成固定的doc tf矩阵，这也是造成速度差距的主要原因。

还有，采用多线程的方式，也可以大大缩短时间。Python由于GIS的原因，只能实现多进程。在利用库进行计算时，可以看到CPU占用率为400%，即利用了所有的双核四线程的资源，而在自己实现时，却只利用了100%。

后续工作可以统计自己在计算过程中每一步的代价，找到代价相对大的步骤进行优化。